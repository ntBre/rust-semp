# -*- org-todo-keyword-faces: (("CHECKPOINT" . "blue") ("WAIT" . "#fce803")); -*-
#+TODO: TODO WAIT | DONE
#+TODO: | CHECKPOINT

this time write input as needed, not all at once. the first three all need to
be inside of the 'as needed' part -> iterator returning jobs?
- if I can set up a list of structs containing all the info necessary for a
  job, I can map over that and submit as I go
- use Iter.take(n) to grab the first n, I guess you slice off the front n
  elements of the original slice after that

see [[https://msfjarvis.dev/posts/building-static-rust-binaries-for-linux/][this]] if I have trouble with static binaries

* tasks
** DONE load geometries
** DONE write .mop input files
** DONE write param file
** DONE write submission scripts
** DONE process MOPAC output
** DONE submit submission scripts
   - need real submission and also test implementation
   - both in place but both need to be tested
** DONE run mopac
   - assemble jobs and run them
** DONE build, run, poll loop
   - take a list of jobs to run and build, run, poll, and clean them
     - for faster calculations (eg the NumJac) just send it more built jobs to
       run at once
   - each piece is ready, just need the loop connecting them
     - probably also factor out the pieces
** DONE clean up after above as stuff finishes
   - partly in place, at least with the struct fields. just need to actually set
     up the structure to do the deleting and call it
** CHECKPOINT above are the essentials
** DONE assemble Jacobian matrix
   build_jobs needs to take a dst argument that's a slice of f64 as well as a
   starting index, and a coefficient.
   - doesn't actually need to take =dst=, only drain needs to take dst
   - would be nice to connect dst to the indices somehow
   - for current use
     - dst is the size of moles
     - starting index is always 0
     - coefficient is always 1
   - for numjac
     - dst will be the size of moles × params
     - starting index will depend on the "column"" I'm doing
       - I'll compute jacᵀ so that each "column" will really be a row and be
         adjacent in one big slice
     - coefficient for forward is 1/2Δ and for back is –1/2Δ

   - probably need the count in build_jobs to be a static so it can increase
     across calls, otherwise could get name collisions when building one part of
     jacobian at a time
     - same with chunk_num in drain
   - how to build jacobian at one time? right now drain points into one
     destination generated by build_chunk
   - let's make dst an argument and give each index a coeff so in the front/back
     parts of numjac I can just use that
** DONE load energies
** TODO set up levmar problem and do matrix math
** TODO implement levmar - cases, lambda stuff, and iteration
** TODO Broyden
** CHECKPOINT below are niceties
** TODO read config file

* basic job-running outline
** generate list of Mopacs
** as needed, turn mopacs into jobs
   - write input file
   - write param file
   - write submission script
   - the last two of these need to be chunked and chunked separately
     - only need to write a new param file when the parameters change
       - share it across forward/back of numjac and across a run of single
         semi-empirical vector run
     - write a new submission script per chunksize jobs
** submit these jobs
** poll finishing jobs and extract energies
** delete finished jobs
   - mop, out, aux, arc
   - param file - when shared jobs done
   - pbs file - when different set of shared jobs done

* running jobs brainstorming
  - the basic operation is run a set of jobs with a single set of parameters
    1. this covers one SE step - f(β)
    2. also covers NumJac - one set of parameters for each front/back per column,
       but then you have to wait for each front/back to finish before you run
       another
  - the reason I want to mix parameters is because of point 2 above - I want to
    be able to pull from the whole Jacobian at one time instead of switching
    from running to writing over and over
    - [X] just go back to one param file per job
  - [X] I think a Job type needs to contain
    1. a Mopac - all the information for setting up the mopac job
       - set when I first generate the jobs
    2. a submit_script - the PBS file for the chunk it's in
       - set when I write the jobs to disk
    3. a job_id - the jobid for the chunk it's in
       - set when the job is submitted
  - cleanup
    - delete Job.mopac.filename.{mop,out,aux,arc} and Job.mopac.paramfile when a
      single job finishes
    - delete Job.submit_script when all the jobs in a chunk finish
      - have to keep track of the jobs belonging to a chunk
